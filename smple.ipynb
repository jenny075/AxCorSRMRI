{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time-_28_05_2024_02_43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import distutils.version\n",
    "from PreprocessUtils import ResampleCases,CreateDateBase\n",
    "from main import *\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T23:44:57.357755600Z",
     "start_time": "2024-05-27T23:44:52.898180Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Date Preprocess\n",
    "For first time users please prepare your data accordingly. Open new folder where case id for both axial and coronal files is the same and its the begining of the file name. THe difference between axial and coronal files should appear in the file name - Axial: \"AXIAL\"/\"Axial\"/\"axial\"/etc.. Coronal:\"CORONAL\"/\"Coronal\"/\"coronal\"/etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Resample isotropicly nifti coronal files\n",
    "path_to_data_files = \"/tcmldrive/shared/RambamMRE082022/new2/\"\n",
    "coronal_files_prefix = None # not madatory\n",
    "ResampleCases(path_dir = path_to_data_files ,prefix = coronal_files_prefix)\n",
    "\n",
    "# Creat DB file with isotropic, coronal and axial files paths\n",
    "CreateDateBase(path_to_data_files,cor_prefix=\"\",ax_prefix=\"\",train_frac=0.8,test_frac=0.1,num_folds = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Param Initializaion\n",
    "\n",
    "Here you can define the main parameters for the model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "override_args = {\n",
    "    \"path_to_set\":\"/tcmldrive/shared/RambamMRE082022/new2/\",\n",
    "    \"path_to_results\":\"/argusdata/users/jenny075/JennySh/results/\",\n",
    "    \"amount_of_files\":20,\n",
    "    \"batch_size\":12,\n",
    "    \"loss\":\"L2\",\n",
    "    \"gpu_device\":\"0,1\",\n",
    "    \"amount_of_slices\":3,\n",
    "    \"title\":\"Test\",\n",
    "    \"total_samples\":100,\n",
    "    \"patch_size\":48,\n",
    "    \"epochs\": 20,\n",
    "    \"lr_g\":0.0001,\n",
    "    \"lr_d\": 0.0001,\n",
    "    \"scheduler\": \"const\",\n",
    "    \"max_workers_train\": 12,\n",
    "    \"max_workers_valid\": 30,\n",
    "    \"random_patches\": True,\n",
    "    \"valid_batch_size\": 40,\n",
    "    \"adversarial_weight_I\": 0.02,\n",
    "    \"adversarial_weight_E\": 0.02,\n",
    "    \"augmentation_state\": True,\n",
    "    \"d_optimizer_step_size\":40 ,\n",
    "    \"g_optimizer_step_size\": 160,\n",
    "    \"val_epoch\": 5,\n",
    "    \"image_save_freq_batch\": 100,\n",
    "    \"mage_save_freq_epoch\": 5,\n",
    "    \"augmentation_state\": True,\n",
    "    \"save_tensor\":True,\n",
    "    \"save_nifti\":True,\n",
    "\n",
    "}\n",
    "\n",
    "parser = setup_parser()\n",
    "args, _ = parser.parse_known_args([])\n",
    "vars(args).update(override_args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T23:45:01.499129200Z",
     "start_time": "2024-05-27T23:45:01.483531Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Creation\n",
    "\n",
    "Create train,valid and test datasets for the model training. In addition the function creates new folder in \"path_to_results\" to save the results and writer for tensorboard tracking.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "config.multi_gpu -  [0, 1]\n",
      "gpu available: True\n",
      "gpu count: 2\n",
      "config.resume -  False\n",
      "Building and creating ESRT model from scratch\n",
      "Load train dataset and valid dataset...\n",
      "path_to_db- /tcmldrive/shared/RambamMRE082022/new2/DB.csv\n",
      "length train_list -  100\n",
      "length val_list -  2\n",
      "length test_list -  2\n",
      "length train_list volume -  2\n",
      "length val_list volume -  2\n",
      "length test_list volume -  2\n",
      "Load train dataset and valid dataset...\n",
      "Load train dataset and valid dataset successfully.\n",
      "Finish data peparation and parameters Initializaion\n"
     ]
    }
   ],
   "source": [
    "dl_train , dl_valid_lr,dl_valid_hr,dl_test_lr,dl_test_hr,result_dir,writer,config  = Data_Inittializaion(args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T23:46:51.152368500Z",
     "start_time": "2024-05-27T23:45:06.951267400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train validation and test\n",
    "initalize the model and model parameters. Afterwords train it on dl_train and validate it on dl_valid_lr,dl_valid_hr. Afetr training is done it is tested dl_test_lr,dl_test_hr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "scale 0.16666666666666666\n",
      "288\n",
      "36\n",
      "Build model successfully.\n",
      "Define all optimizer functions...\n",
      "Define all optimizer functions successfully.\n",
      "Define all optimizer scheduler functions...\n",
      "Define all optimizer scheduler functions successfully.\n",
      "Define all loss functions...\n",
      "Define all loss functions successfully.\n",
      "Check whether the training weight is restored...\n",
      "Check whether the training weight is restored successfully.\n",
      "Start train the model.\n",
      "worker ID-0,Dataset_start-0,Dataset_end-8,per_worker-8\n",
      "worker ID-1,Dataset_start-8,Dataset_end-16,per_worker-8\n",
      "worker ID-2,Dataset_start-16,Dataset_end-24,per_worker-8\n",
      "worker ID-3,Dataset_start-24,Dataset_end-32,per_worker-8\n",
      "worker ID-4,Dataset_start-32,Dataset_end-40,per_worker-8\n",
      "worker ID-5,Dataset_start-40,Dataset_end-48,per_worker-8\n",
      "worker ID-6,Dataset_start-48,Dataset_end-56,per_worker-8\n",
      "worker ID-7,Dataset_start-56,Dataset_end-64,per_worker-8\n",
      "worker ID-8,Dataset_start-64,Dataset_end-72,per_worker-8worker ID-9,Dataset_start-72,Dataset_end-80,per_worker-8\n",
      "\n",
      "worker ID-10,Dataset_start-80,Dataset_end-88,per_worker-8\n",
      "worker ID-11,Dataset_start-88,Dataset_end-96,per_worker-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tcmldrive/JennySh/conda_envs/MRI_SR/lib/python3.8/site-packages/torch/nn/modules/conv.py:439: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /pytorch/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/tcmldrive/JennySh/conda_envs/MRI_SR/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/tcmldrive/JennySh/conda_envs/MRI_SR/lib/python3.8/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train time- 00:00:06\n",
      "Start Vlidation\n",
      "LR max size [280, 512] max patches 153\n",
      "HR max size [512, 512] max patches 289\n",
      "Start HR reconstract\n",
      "Finish reconstrct HR 00:00:06\n",
      "Start LR reconstract\n"
     ]
    }
   ],
   "source": [
    "training_validation_test(dl_train , dl_valid_lr,dl_valid_hr,dl_test_lr,dl_test_hr,result_dir,writer,config)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-27T23:46:51.152368500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
